{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c2a5bb",
   "metadata": {},
   "source": [
    "# Etapa 3 - Limpieza de Datos\n",
    "\n",
    "**Proyecto:** Ciencia de Datos - Preparaci√≥n de Datos  \n",
    "**Universidad:** Pontificia Universidad Javeriana  \n",
    "**Curso:** Tecnolog√≠as Emergentes 2025  \n",
    "**Profesor:** Luis Carlos Chica√≠za\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo de esta Etapa\n",
    "\n",
    "Realizar la **limpieza sistem√°tica** del dataset de accidentes de tr√°nsito en Bucaramanga, abordando:\n",
    "\n",
    "1. Reconocimiento y tratamiento de atributos con valores √∫nicos o casi √∫nicos.  \n",
    "2. An√°lisis y tratamiento de valores faltantes.  \n",
    "3. Identificaci√≥n y tratamiento de valores at√≠picos o inconsistentes.  \n",
    "4. Generaci√≥n de un dataset limpio y documentado para la etapa 4 (vista minable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4e6dc",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b0371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importar librer√≠as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from src.limpieza_datos import (\n",
    "    cargar_datos,\n",
    "    diagnostico_calidad,\n",
    "    tratar_atributos_unicos,\n",
    "    tratar_valores_faltantes,\n",
    "    tratar_atipicos_iqr,\n",
    "    normalizar_texto_categorico\n",
    ")\n",
    "\n",
    "# Configuraci√≥n\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas y configuradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3d16d",
   "metadata": {},
   "source": [
    "## 2. Carga del Dataset y Diagn√≥stico Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537df979",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cargar dataset original (mismas rutas usadas en las etapas 1 y 2)\n",
    "file_path = '../data/raw/accidentes_transito.csv'\n",
    "\n",
    "# Intentar cargar con diferentes encodings si es necesario\n",
    "try:\n",
    "    df_raw = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df_raw = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET ORIGINAL - INFORMACI√ìN B√ÅSICA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Registros: {len(df_raw):,}\")\n",
    "print(f\"Atributos: {len(df_raw.columns)}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\nInformaci√≥n general:\")\n",
    "print(df_raw.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c556b",
   "metadata": {},
   "source": [
    "### 2.1 Tabla de Diagn√≥stico de Calidad de Datos\n",
    "\n",
    "Como primer paso, se construye una tabla de diagn√≥stico por atributo, que resume:\n",
    "\n",
    "- Tipo de dato.  \n",
    "- N√∫mero y porcentaje de valores faltantes.  \n",
    "- Cardinalidad (n√∫mero de valores distintos).  \n",
    "- Ejemplos de valores observados.\n",
    "\n",
    "Esto permite identificar r√°pidamente columnas con muchos nulos, alta unicidad o posibles problemas de calidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537849d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Construir tabla de diagn√≥stico usando la funci√≥n auxiliar\n",
    "tabla_diagnostico = diagnostico_calidad(df_raw)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLA DE DIAGN√ìSTICO DE CALIDAD - VISTA GENERAL\")\n",
    "print(\"=\" * 80)\n",
    "display(tabla_diagnostico)\n",
    "\n",
    "print(\"\\nResumen de valores faltantes por columna:\")\n",
    "cols_con_nulos = tabla_diagnostico[tabla_diagnostico['n_nulos'] > 0]\n",
    "if not cols_con_nulos.empty:\n",
    "    for _, row in cols_con_nulos.iterrows():\n",
    "        print(f\" - {row['columna']}: {row['n_nulos']:,} nulos ({row['pct_nulos']}%)\")\n",
    "else:\n",
    "    print(\"No se encontraron valores nulos en las columnas analizadas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfa2fb",
   "metadata": {},
   "source": [
    "## 3. Tratamiento de Atributos con Valores √önicos o Casi √önicos\n",
    "\n",
    "En esta secci√≥n se identifican columnas que:\n",
    "\n",
    "- Funcionan como identificadores puros (por ejemplo, `ORDEN`).  \n",
    "- Tienen una **cardinalidad muy alta** (casi un valor distinto por fila).\n",
    "\n",
    "Las columnas que act√∫an como ID se eliminan del an√°lisis; las de alta unicidad se documentan y se decide si se conservan por su relevancia anal√≠tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7c038",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar tratamiento de atributos √∫nicos / casi √∫nicos\n",
    "df_unicos, decisiones_unicos = tratar_atributos_unicos(df_raw)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRATAMIENTO DE ATRIBUTOS √öNICOS / CASI √öNICOS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Decisiones tomadas:\")\n",
    "for col, decision in decisiones_unicos.items():\n",
    "    print(f\" - {col}: {decision}\")\n",
    "\n",
    "print(\"\\nForma del dataset antes / despu√©s:\")\n",
    "print(f\" - Original: {df_raw.shape[0]:,} filas, {df_raw.shape[1]} columnas\")\n",
    "print(f\" - Tras tratamiento de unicidad: {df_unicos.shape[0]:,} filas, {df_unicos.shape[1]} columnas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a068b",
   "metadata": {},
   "source": [
    "## 4. Tratamiento de Valores Faltantes\n",
    "\n",
    "A partir de la tabla de diagn√≥stico, se definen criterios para el manejo de nulos:\n",
    "\n",
    "- Columnas con m√°s del **50% de valores faltantes** se eliminan.  \n",
    "- En el resto:\n",
    "  - Variables num√©ricas: imputaci√≥n con la **mediana**.  \n",
    "  - Variables categ√≥ricas: imputaci√≥n con la **moda** (valor m√°s frecuente).\n",
    "\n",
    "Este criterio busca un balance entre conservar informaci√≥n y evitar sesgos excesivos por imputaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92789fb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar tratamiento de valores faltantes\n",
    "df_nulos, decisiones_nulos = tratar_valores_faltantes(df_unicos)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRATAMIENTO DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Decisiones por columna (muestra):\")\n",
    "for col, decision in list(decisiones_nulos.items())[:30]:\n",
    "    print(f\" - {col}: {decision}\")\n",
    "\n",
    "print(\"\\nN√∫mero de columnas eliminadas por alto porcentaje de nulos:\")\n",
    "eliminadas = [c for c, d in decisiones_nulos.items() if 'eliminada' in d]\n",
    "print(f\" - Total columnas eliminadas: {len(eliminadas)}\")\n",
    "if eliminadas:\n",
    "    print(\"   Columnas eliminadas:\", ', '.join(eliminadas))\n",
    "\n",
    "print(\"\\nForma del dataset tras tratamiento de nulos:\")\n",
    "print(f\" - Filas: {df_nulos.shape[0]:,}\")\n",
    "print(f\" - Columnas: {df_nulos.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224a0e9",
   "metadata": {},
   "source": [
    "## 5. Normalizaci√≥n de Variables Categ√≥ricas\n",
    "\n",
    "Se realiza una limpieza b√°sica de texto en variables categ√≥ricas clave:\n",
    "\n",
    "- Eliminaci√≥n de espacios en blanco al inicio y al final.  \n",
    "- Unificaci√≥n de espacios intermedios m√∫ltiples.\n",
    "\n",
    "Esto ayuda a reducir problemas derivados de valores como `\" Norte \"` vs `\"Norte\"` o variaciones similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0933c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Definir columnas categ√≥ricas a normalizar (seg√∫n exploraci√≥n previa)\n",
    "columnas_texto = [\n",
    "    \"BARRIO\",\n",
    "    \"COMUNA\",\n",
    "    \"GRAVEDAD\",\n",
    "    \"MES\",\n",
    "    \"D√çA\",\n",
    "    \"ENTIDAD\",\n",
    "    \"Propietario de Veh√≠culo\",\n",
    "    \"DIURNIO/NOCTURNO\",\n",
    "]\n",
    "\n",
    "df_cat = normalizar_texto_categorico(df_nulos, columnas_texto)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NORMALIZACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Columnas normalizadas:\")\n",
    "for col in columnas_texto:\n",
    "    if col in df_cat.columns:\n",
    "        print(f\" - {col}\")\n",
    "    else:\n",
    "        print(f\" - {col} (no presente en el DataFrame tras pasos anteriores)\")\n",
    "\n",
    "# Ejemplo de cambio en una columna categ√≥rica\n",
    "if \"GRAVEDAD\" in df_cat.columns:\n",
    "    print(\"\\nEjemplo de valores en GRAVEDAD tras normalizaci√≥n:\")\n",
    "    print(df_cat[\"GRAVEDAD\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac415ad4",
   "metadata": {},
   "source": [
    "## 6. Tratamiento de Valores At√≠picos (Outliers) en Variables Num√©ricas\n",
    "\n",
    "Para las variables num√©ricas se aplican reglas basadas en el **rango intercuart√≠lico (IQR)**:\n",
    "\n",
    "- Se calculan Q1, Q3 e IQR = Q3 ‚àí Q1.  \n",
    "- Se fijan l√≠mites inferior y superior:  \n",
    "  \\\\( \\\\text{L√≠mite Inf} = Q1 - 1.5 \\\\times IQR \\\\),  \n",
    "  \\\\( \\\\text{L√≠mite Sup} = Q3 + 1.5 \\\\times IQR \\\\).  \n",
    "- Los valores que quedan por fuera se **recortan** (winsorizaci√≥n) a dichos l√≠mites.\n",
    "\n",
    "Con esto se reduce el impacto de registros extremos sin eliminarlos por completo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508efb34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar tratamiento de valores at√≠picos num√©ricos\n",
    "df_limpio, resumen_atipicos = tratar_atipicos_iqr(df_cat)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRATAMIENTO DE VALORES AT√çPICOS NUM√âRICOS (IQR)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if resumen_atipicos:\n",
    "    resumen_df = pd.DataFrame.from_dict(resumen_atipicos, orient='index')\n",
    "    display(resumen_df)\n",
    "    print(\"\\nColumnas con valores modificados (n_modificados > 0):\")\n",
    "    modificadas = resumen_df[resumen_df['n_modificados'] > 0]\n",
    "    if not modificadas.empty:\n",
    "        for col, row in modificadas.iterrows():\n",
    "            print(f\" - {col}: {int(row['n_modificados'])} registros recortados\")\n",
    "    else:\n",
    "        print(\"No se detectaron outliers seg√∫n el criterio IQR.\")\n",
    "else:\n",
    "    print(\"No se gener√≥ resumen de at√≠picos (posiblemente no hay columnas num√©ricas con variaci√≥n suficiente).\")\n",
    "\n",
    "print(\"\\nForma final del dataset limpio:\")\n",
    "print(f\" - Filas: {df_limpio.shape[0]:,}\")\n",
    "print(f\" - Columnas: {df_limpio.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f109f0",
   "metadata": {},
   "source": [
    "## 7. Exportaci√≥n del Dataset Limpio\n",
    "\n",
    "Finalmente, se guarda el resultado de la limpieza en la carpeta `data/processed/`, para su uso en la etapa 4 (construcci√≥n de vista minable y an√°lisis posterior).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae7842",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Guardar dataset limpio\n",
    "output_path = '../data/processed/accidentes_transito_limpio.csv'\n",
    "df_limpio.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTACI√ìN DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Archivo guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5b793",
   "metadata": {},
   "source": [
    "## 8. Conclusiones de la Etapa 3 - Limpieza de Datos\n",
    "\n",
    "### ‚úÖ Resumen de Transformaciones Clave\n",
    "\n",
    "- **Atributos √∫nicos / casi √∫nicos**  \n",
    "  - Se elimin√≥ la columna de identificaci√≥n `ORDEN`, al no aportar informaci√≥n anal√≠tica.  \n",
    "  - Se documentaron columnas de alta unicidad (por ejemplo, fechas) que se conservaron por su relevancia temporal.\n",
    "\n",
    "- **Valores faltantes**  \n",
    "  - Se eliminaron columnas con m√°s del 50% de valores faltantes (criterio de baja confiabilidad).  \n",
    "  - En el resto de atributos:\n",
    "    - Variables num√©ricas: imputaci√≥n con la mediana.  \n",
    "    - Variables categ√≥ricas: imputaci√≥n con la moda o un valor marcador (`SIN_DATO`) cuando fue necesario.\n",
    "\n",
    "- **Valores at√≠picos**  \n",
    "  - Se aplic√≥ el m√©todo IQR con winsorizaci√≥n en variables num√©ricas, reduciendo el impacto de registros extremos sin eliminar observaciones completas.\n",
    "\n",
    "- **Normalizaci√≥n de texto**  \n",
    "  - Se unificaron formatos en variables categ√≥ricas clave (`GRAVEDAD`, `BARRIO`, `COMUNA`, `DIURNIO/NOCTURNO`, entre otras), reduciendo la dispersi√≥n artificial de categor√≠as por diferencias de escritura.\n",
    "\n",
    "### üéØ Impacto en el Proyecto\n",
    "\n",
    "- El dataset resultante presenta **mejor consistencia y calidad**, lo cual:\n",
    "  - Facilita la interpretaci√≥n de patrones en las etapas siguientes.  \n",
    "  - Reduce el riesgo de conclusiones sesgadas por errores de captura o datos extremos.  \n",
    "  - Deja documentadas las decisiones de limpieza, lo que aporta transparencia y reproducibilidad al proyecto.\n",
    "\n",
    "Con esta etapa, se completa el macroproceso de **limpieza de datos**, dejando el dataset listo para la construcci√≥n de la **vista minable** y el an√°lisis m√°s profundo en la Etapa 4.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
